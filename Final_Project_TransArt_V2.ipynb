{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Now Let Try the same For Translating app Tamil to English"
      ],
      "metadata": {
        "id": "21-d_Dkn6XDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*First lets try with 3 different model and Pick one best model out of it*"
      ],
      "metadata": {
        "id": "-cupdv5R6alb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Set the Hugging Face API URL and token\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt\"\n",
        "HUGGINGFACE_TOKEN = \"Token\"\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
        "\n",
        "# Define the text to be translated (Tamil to English)\n",
        "text = \"‡Æö‡ØÜ‡ÆØ‡Æ±‡Øç‡Æï‡Øà ‡Æ®‡ØÅ‡Æ£‡Øç‡Æ£‡Æ±‡Æø‡Æµ‡ØÅ\"\n",
        "\n",
        "# Create the payload\n",
        "payload = {\n",
        "    \"inputs\": text\n",
        "}\n",
        "\n",
        "# Make the request to the API\n",
        "response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the response\n",
        "    result = response.json()\n",
        "    translated_text = result[0]['generated_text']\n",
        "    print(\"Translated Text:\", translated_text)\n",
        "else:\n",
        "    print(f\"Error {response.status_code}: {response.text}\")\n"
      ],
      "metadata": {
        "id": "hZEyU95d6ZVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dde47e5-75f4-42c5-c739-1403ee4ada73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text: Artificial intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **I have tried with 5 different model but those model are not working properly and some model are not loading**\n",
        "\n",
        "*abhinand/gemma-2b-it-tamil-v0.1-alpha*\n",
        "\n",
        "*Helsinki-NLP/opus-mt-mul-en*\n",
        "\n",
        "*google-t5/t5-small*\n",
        "\n",
        "*google/mt5-small*\n",
        "\n",
        "*google/mt5-xl*"
      ],
      "metadata": {
        "id": "trr_zB0uOMtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Creating the Text to text generation Model Using Groq API"
      ],
      "metadata": {
        "id": "DLSQYptXvSlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lED3zrnKwDIO",
        "outputId": "a9ca2c87-e140-4c76-d7d0-11c2d602bf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Groq API Limitations  \n",
        "\n",
        "Requests per minute: 30\n",
        "\n",
        "Requests per day: 14,400\n",
        "\n",
        "Tokens per minute: 6,000\n",
        "\n",
        "Input Token Limit: 32,768 tokens"
      ],
      "metadata": {
        "id": "-vmmve8Jzq7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets try Different Model and check Which giving a educational and entertaining content."
      ],
      "metadata": {
        "id": "EkIMeJq-xO5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suBTzxHxr2z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3a4da4-e5c4-4e1a-c4b3-445c0ae7261b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: Artificial Intelligence (AI) is transforming the world. It's like magic, making machines think and learn like humans. But it's not magic, it's science! AI uses algorithms and data to make decisions, solve problems and even create art.\n",
            "\n",
            "Imagine a world where AI helps doctors diagnose diseases, where self-driving cars rule the roads, and where robots assist in our daily lives. This is not the stuff of science fiction, it's the future, and it's happening now!\n",
            "\n",
            "AI is everywhere, from voice assistants like Siri and Alexa, to recommendation algorithms on streaming platforms. It's a rapidly evolving field, with new breakthroughs and applications emerging all the time.\n",
            "\n",
            "But as with any powerful technology, AI also raises important questions about ethics, privacy, and job displacement. As we continue to explore the potential of AI, it's crucial that we also consider its impact on society and work to ensure that it is used responsibly and ethically.\n",
            "\n",
            "So come join the AI revolution! Whether you're a student, a teacher, or a lifelong learner, there's never been a better time to dive into the world of Artificial Intelligence. It's a thrilling ride, full of discovery, innovation, and possibility!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Setting Groq API key directly in the code\n",
        "api_key = \"Token\"\n",
        "\n",
        "# API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request payload for educational and creative content generation\n",
        "payload = {\n",
        "    #\"model\": \"llama3-8b-8192\"\n",
        "    #\"model\": \"llama-3.1-70b-versatile\",  # The model\n",
        "    #\"model\": \"gemma2-9b-it\",\n",
        "    \"model\": \"mixtral-8x7b-32768\",# The model\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative and insightful writer who excels at generating engaging and original educational content. Your style makes learning fun, imaginative, and easy to understand.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a content about Artificical Intellegence with in 300 tokens\"}\n",
        "    ],\n",
        "    \"max_tokens\": 300  # Enough space for educational explanations\n",
        "}\n",
        "\n",
        "# Send POST request to Groq API\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    result = response.json()\n",
        "    # Extract and print the generated text\n",
        "    generated_text = result['choices'][0]['message']['content']\n",
        "    print(\"Generated text:\", generated_text)\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model------> llama3-8b-8192\n",
        "\n",
        "**Generated text:** \"Unleashing the Power of Artificial Intelligence: Unlocking the Secrets to Efficient Learning\"\n",
        "\n",
        "Artificial Intelligence (AI) has revolutionized the way we learn and interact with technology. By harnessing the power of machine learning, AI algorithms can process vast amounts of data, identify patterns, and provide personalized feedback. This cutting-edge technology has already shown remarkable potential in various fields, from healthcare to education.\n",
        "\n",
        "Imagine having a personal tutor that can analyze your strengths, weaknesses, and learning pace, adapting lesson plans to your unique needs. AI-powered educational tools can help students master difficult subjects, build confidence, and stay engaged in learning. With AI, the future of education is looking brighter, more interactive, and more effective than ever.\n",
        "\n",
        "What do you think? Are you fascinated by the possibilities of AI in education? Share your thoughts!\n",
        "\n",
        "##Model ------> gemma2-9b-it\n",
        "\n",
        "**Generated text:** Ever wished you had a robot friend who could learn and grow with you? That's what Artificial Intelligence (AI) is all about!\n",
        "\n",
        "Think of AI like a super-smart computer program. It uses powerful algorithms to learn from tons of data, just like we learn from our experiences.\n",
        "\n",
        "AI can do amazing things: from understanding your voice commands to recommending your next favorite song, even composing its own music!\n",
        "\n",
        "It's helping doctors diagnose diseases, self-driving cars navigate streets, and artists create stunning new works.\n",
        "\n",
        "AI is changing the world around us every day, and it's only getting smarter!\n",
        "\n",
        "##Model ------> llama-3.1-70b-versatile\n",
        "\n",
        "**Generated text:** Unlocking the Power of Artificial Intelligence\n",
        "\n",
        "Imagine a world where machines think, learn, and adapt like humans. Welcome to the realm of Artificial Intelligence (AI). AI simulates human intelligence using algorithms, enabling computers to perform tasks that typically require human intelligence, such as:\n",
        "\n",
        "- Recognizing patterns and images\n",
        "- Understanding natural language\n",
        "- Making decisions and predictions\n",
        "\n",
        "AI is transforming industries like healthcare, finance, and transportation, making our lives more efficient and convenient. From virtual assistants to self-driving cars, AI is revolutionizing the way we live and work.\n",
        "\n",
        "**So, how does AI work?**\n",
        "\n",
        "AI systems are trained on vast amounts of data, which enables them to learn from experiences and improve over time. This process is called machine learning. AI can be classified into two types: Narrow AI (specialized in a specific task) and General AI (mimics human intelligence).\n",
        "\n",
        "**The Future of AI**\n",
        "\n",
        "As AI continues to evolve, we can expect to see significant advancements in areas like robotics, natural language processing, and computer vision. However, with great power comes great responsibility. It's essential to address concerns around AI's impact on jobs, bias, and security.\n",
        "\n",
        "**Join the AI Revolution**\n",
        "\n",
        "As we embark on this exciting journey, we have the power to shape the future of AI. By understanding its potential and limitations, we can harness its benefits and create a better world for all.\n",
        "\n",
        "\n",
        "##Model ------> \"mixtral-8x7b-32768\"\n",
        "\n",
        "**Generated text:** Artificial Intelligence (AI) is transforming the world. It's like magic, making machines think and learn like humans. But it's not magic, it's science! AI uses algorithms and data to make decisions, solve problems and even create art.\n",
        "\n",
        "Imagine a world where AI helps doctors diagnose diseases, where self-driving cars rule the roads, and where robots assist in our daily lives. This is not the stuff of science fiction, it's the future, and it's happening now!\n",
        "\n",
        "AI is everywhere, from voice assistants like Siri and Alexa, to recommendation algorithms on streaming platforms. It's a rapidly evolving field, with new breakthroughs and applications emerging all the time.\n",
        "\n",
        "But as with any powerful technology, AI also raises important questions about ethics, privacy, and job displacement. As we continue to explore the potential of AI, it's crucial that we also consider its impact on society and work to ensure that it is used responsibly and ethically.\n",
        "\n",
        "So come join the AI revolution! Whether you're a student, a teacher, or a lifelong learner, there's never been a better time to dive into the world of Artificial Intelligence. It's a thrilling ride, full of discovery, innovation, and possibility!\n",
        "\n"
      ],
      "metadata": {
        "id": "okPdWR0JzOxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________"
      ],
      "metadata": {
        "id": "FyqKaAR25HNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Picked For Text Generation\n",
        "\n",
        "> Which Model I have picked means **llama-3.1-70b-versatile** why I have picked this means It suits for the content generations thing because It give a Heading wise outputs so there would be a clear understanding out of it so I have Picked This model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PQbub_jy3Wm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> I have decided that Giving a plain text to the image model won't generated a good looking image so I have decided to create a model which can able to create a good text to image prompt\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4T8yk3msi6Y_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da94cc5-6dac-4a90-ef64-1643f333332a",
        "id": "bDf5l2osjeSu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: \"A futuristic scene of artificial intelligence: a room filled with advanced computers and robotics, with a glowing orb representing the AI brain, surrounded by data streams and symbols of machine learning.\"\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Setting Groq API key directly in the code\n",
        "api_key = \"Token\"\n",
        "\n",
        "# API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request payload for educational and creative content generation\n",
        "payload = {\n",
        "    #\"model\": \"llama3-8b-8192\"\n",
        "    #\"model\": \"llama-3.1-70b-versatile\",  # The model\n",
        "    #\"model\": \"gemma2-9b-it\",\n",
        "    \"model\": \"mixtral-8x7b-32768\",# The model\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a realistic,insightful,positive and professional Text to image prompt generator which generate a prompt for image generation\"},\n",
        "        {\"role\": \"user\", \"content\": \"create a text to image generation prompt about Artificical Intellegence with in 30 tokens\"}\n",
        "    ],\n",
        "    \"max_tokens\": 30  # Enough space for educational explanations\n",
        "}\n",
        "\n",
        "# Send POST request to Groq API\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    result = response.json()\n",
        "    # Extract and print the generated text\n",
        "    generated_text = result['choices'][0]['message']['content']\n",
        "    print(\"Generated text:\", generated_text)\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Lets choose the Text to image generation model for Image generation"
      ],
      "metadata": {
        "id": "mOnF3wvZWWTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Set the API URL and the Hugging Face token\n",
        "\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/CompVis/stable-diffusion-v1-4\"\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
        "HUGGINGFACE_TOKEN = \"Token\"\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Input text prompt for generating the image\n",
        "text_prompt = \"Create an image of a futuristic robot interacting with humans in a modern city, showcasing advanced AI technology\"\n",
        "\n",
        "# Payload for the API request\n",
        "data = {\n",
        "    \"inputs\": text_prompt\n",
        "}\n",
        "\n",
        "# Query the API with the text prompt\n",
        "image_data = query(data)\n",
        "\n",
        "# If the response contains image data, display it\n",
        "if image_data:\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "    image.show()  # Opens the image in the default viewer\n",
        "    image.save(\"generated_image.png\")  # Optionally save the image\n"
      ],
      "metadata": {
        "id": "EziEnaq-WvJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Image Generated By Flux_1 ----> /content/Flux_1.png\n",
        "\n",
        "> Image Generated By stable-diffusion ------>  /content/stable-diffusion.png\n",
        "\n",
        ">Image Generated By Flux_1_schnell -----> /content/Flux_1_schnell.png\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSjvCQfCdO48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So Out of this three model Flux_1_schnell is working good eventhough all are good I have choosen this since It uses the CPU and have a lower steps.**"
      ],
      "metadata": {
        "id": "WYw8DeXih50w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Lets Combine all the model together and check whether all the model are perfectly working or not when combines together"
      ],
      "metadata": {
        "id": "62RG6TitiMVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Hugging Face API Token\n",
        "HUGGINGFACE_TOKEN = \"Token\"\n",
        "headers_hf = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
        "\n",
        "# Groq API Token\n",
        "GROQ_API_KEY = \"Token\"\n",
        "headers_groq = {\n",
        "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Translation Model API URL (Tamil to English)\n",
        "translation_url = \"https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt\"\n",
        "\n",
        "# Text-to-Image Model API URL\n",
        "image_generation_url = \"https://api-inference.huggingface.co/models/CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# Function to query Hugging Face translation model\n",
        "def translate_text(text):\n",
        "    payload = {\"inputs\": text}\n",
        "    response = requests.post(translation_url, headers=headers_hf, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        translated_text = result[0]['generated_text']\n",
        "        return translated_text\n",
        "    else:\n",
        "        print(f\"Translation Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Function to query Groq content generation model\n",
        "def generate_content(english_text):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative and insightful writer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Write educational content about {english_text} within 300 tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": 300\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=headers_groq)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        print(f\"Content Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate image prompt\n",
        "def generate_image_prompt(english_text):\n",
        "    payload = {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a professional Text to image prompt generator.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Create a text to image generation prompt about {english_text} within 30 tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": 30\n",
        "    }\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", json=payload, headers=headers_groq)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        print(f\"Prompt Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate an image from the prompt\n",
        "def generate_image(image_prompt):\n",
        "    data = {\"inputs\": image_prompt}\n",
        "    response = requests.post(image_generation_url, headers=headers_hf, json=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        print(f\"Image Generation Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Main function to handle multimodal pipeline\n",
        "def multimodal_pipeline():\n",
        "    # Step 1: Get Tamil input\n",
        "    tamil_input = input(\"Enter Tamil text (or press Enter to input in English): \").strip()\n",
        "\n",
        "    if tamil_input:\n",
        "        # Step 2: Attempt translation\n",
        "        english_text = translate_text(tamil_input)\n",
        "        if english_text:\n",
        "            print(\"Translated Text:\", english_text)\n",
        "        else:\n",
        "            # Translation failed, ask for English input\n",
        "            english_text = input(\"Translation failed. Please enter English input: \").strip()\n",
        "    else:\n",
        "        # No Tamil input provided, ask for English input\n",
        "        english_text = input(\"Please enter English input: \").strip()\n",
        "\n",
        "    # Step 3: Generate educational content\n",
        "    content_output = generate_content(english_text)\n",
        "    print(\"Content Generation Output:\", content_output)\n",
        "\n",
        "    # Step 4: Generate image prompt and image\n",
        "    image_prompt = generate_image_prompt(english_text)\n",
        "    image_data = generate_image(image_prompt)\n",
        "\n",
        "    if image_data:\n",
        "        # Step 5: Display and save the image\n",
        "        image = Image.open(BytesIO(image_data))\n",
        "        image.show()  # Opens the image in the default viewer\n",
        "        image.save(\"generated_image.png\")\n",
        "\n",
        "# Run the multimodal pipeline\n",
        "multimodal_pipeline()\n"
      ],
      "metadata": {
        "id": "O6eztbSyijKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674931e2-a359-4318-fac6-e3b455a5b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Tamil text (or press Enter to input in English): ‡Æ§‡Æ∞‡Æµ‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\n",
            "Translated Text: Data science\n",
            "Content Generation Output: Title: Unlocking the Power of Data Science: A Beginner's Guide\n",
            "\n",
            "Data science is a multidisciplinary field that combines mathematics, statistics, and computer science to extract knowledge and insights from structured and unstructured data. It has become a critical tool for businesses, governments, and researchers to make informed decisions, improve processes, and drive innovation.\n",
            "\n",
            "At its core, data science involves three key steps: data collection, data analysis, and data visualization. Here's a closer look at each step:\n",
            "\n",
            "1. Data Collection:\n",
            "Data science begins with collecting and gathering data from various sources such as databases, APIs, web pages, or even sensors and IoT devices. Data can come in many formats, including structured data (e.g., spreadsheets, SQL databases) and unstructured data (e.g., text documents, images, videos). Effective data collection strategies help ensure that the data is accurate, complete, and relevant to the problem or question being addressed.\n",
            "\n",
            "2. Data Analysis:\n",
            "Once data is collected, data scientists use statistical and machine learning techniques to analyze and uncover patterns and insights within the data. Data analysis can help answer questions, test hypotheses, or identify relationships between variables. Some common data analysis techniques include:\n",
            "\n",
            "* Descriptive statistics (mean, median, mode, standard deviation)\n",
            "* Inferential statistics (hypothesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actuall Fully working deployed code"
      ],
      "metadata": {
        "id": "xQ4gwruBji8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Fetch Hugging Face and Groq API keys from secrets\n",
        "Transalate_token = os.getenv('Translate')\n",
        "Image_Token = os.getenv('Image_generation')\n",
        "Content_Token = os.getenv('ContentGeneration')\n",
        "Image_prompt_token = os.getenv('Prompt_generation')\n",
        "\n",
        "# API Headers\n",
        "Translate = {\"Authorization\": f\"Bearer {Transalate_token}\"}\n",
        "Image_generation = {\"Authorization\": f\"Bearer {Image_Token}\"}\n",
        "Content_generation = {\n",
        "    \"Authorization\": f\"Bearer {Content_Token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "Image_Prompt = {\n",
        "    \"Authorization\": f\"Bearer {Image_prompt_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Translation Model API URL (Tamil to English)\n",
        "translation_url = \"https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt\"\n",
        "\n",
        "# Text-to-Image Model API URL\n",
        "image_generation_url = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\"\n",
        "\n",
        "# Function to query Hugging Face translation model\n",
        "def translate_text(text):\n",
        "    payload = {\"inputs\": text}\n",
        "    response = requests.post(translation_url, headers=Translate, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        translated_text = result[0]['generated_text']\n",
        "        return translated_text\n",
        "    else:\n",
        "        st.error(f\"Translation Error {response.status_code}: {response.text}\")\n",
        "        st.write(f'Please try after sometime üò•üò•üò•')\n",
        "        return None\n",
        "\n",
        "# Function to query Groq content generation model\n",
        "def generate_content(english_text, max_tokens, temperature):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.1-70b-versatile\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative and insightful writer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Write educational content about {english_text} within {max_tokens} tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=Content_generation)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Content Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate image prompt\n",
        "def generate_image_prompt(english_text):\n",
        "    payload = {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a professional Text to image prompt generator.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Create a text to image generation prompt about {english_text} within 30 tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": 30\n",
        "    }\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", json=payload, headers=Image_Prompt)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Prompt Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate an image from the prompt\n",
        "def generate_image(image_prompt):\n",
        "    data = {\"inputs\": image_prompt}\n",
        "    response = requests.post(image_generation_url, headers=Image_generation, json=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        st.error(f\"Image Generation Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    # Custom CSS for background, borders, and other styling\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        body {\n",
        "            background-image: url('https://wallpaperaccess.com/full/1567666.png');\n",
        "            background-size: cover;\n",
        "        }\n",
        "        .reportview-container {\n",
        "            background: rgba(255, 255, 255, 0.85);\n",
        "            padding: 2rem;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        .result-container {\n",
        "            border: 2px solid #4CAF50;\n",
        "            padding: 20px;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 20px;\n",
        "            animation: fadeIn 2s ease;\n",
        "        }\n",
        "        @keyframes fadeIn {\n",
        "            0% { opacity: 0; }\n",
        "            100% { opacity: 1; }\n",
        "        }\n",
        "        .stButton button {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "            border-radius: 10px;\n",
        "            padding: 10px;\n",
        "        }\n",
        "        .stButton button:hover {\n",
        "            background-color: #45a049;\n",
        "            transform: scale(1.05);\n",
        "            transition: 0.2s ease-in-out;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\", unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    st.title(\"üÖ∞Ô∏è‚ÑπÔ∏è FusionMind ‚û°Ô∏è Multimodal\")\n",
        "\n",
        "    # Sidebar for temperature and token adjustment\n",
        "    st.sidebar.header(\"Settings\")\n",
        "    temperature = st.sidebar.slider(\"Select Temperature\", 0.1, 1.0, 0.7)\n",
        "    max_tokens = st.sidebar.slider(\"Max Tokens for Content Generation\", 100, 400, 200)\n",
        "\n",
        "    # Suggested inputs\n",
        "    st.write(\"## Suggested Inputs\")\n",
        "    suggestions = [\"‡Æ§‡Æ∞‡Æµ‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\", \"‡Æ™‡ØÅ‡Æ§‡Æø‡ÆØ ‡Æ§‡Æø‡Æ±‡Æ©‡Øç‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Øç‡Æï‡Øä‡Æ≥‡Øç‡Æµ‡Æ§‡ØÅ ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø\", \"‡Æ∞‡Ææ‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æµ‡Øá‡Æ≤‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\"]\n",
        "    selected_suggestion = st.selectbox(\"Select a suggestion or enter your own:\", [\"\"] + suggestions)\n",
        "\n",
        "    # Input box for user\n",
        "    tamil_input = st.text_input(\"Enter Tamil text (or select a suggestion):\", selected_suggestion)\n",
        "\n",
        "    if st.button(\"Generate\"):\n",
        "        # Step 1: Translation (Tamil to English)\n",
        "        if tamil_input:\n",
        "            st.write(\"### Translated English Text:\")\n",
        "            english_text = translate_text(tamil_input)\n",
        "            if english_text:\n",
        "                st.success(english_text)\n",
        "\n",
        "                # Step 2: Generate Educational Content\n",
        "                st.write(\"### Generated Educational Content:\")\n",
        "                with st.spinner('Generating content...'):\n",
        "                    content_output = generate_content(english_text, max_tokens, temperature)\n",
        "                    if content_output:\n",
        "                        st.success(content_output)\n",
        "\n",
        "                # Step 3: Generate Image from the prompt\n",
        "                st.write(\"### Generated Image:\")\n",
        "                with st.spinner('Generating image...'):\n",
        "                    image_prompt = generate_image_prompt(english_text)\n",
        "                    image_data = generate_image(image_prompt)\n",
        "                    if image_data:\n",
        "                        st.image(image_data, caption=\"Generated Image\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qPV226ljjtst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Multimodel Selection Code"
      ],
      "metadata": {
        "id": "uQsOR3VseptM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Fetch Hugging Face and Groq API keys from secrets\n",
        "Transalate_token = os.getenv('Translate')\n",
        "Image_Token = os.getenv('Image_generation')\n",
        "Content_Token = os.getenv('ContentGeneration')\n",
        "Image_prompt_token = os.getenv('Prompt_generation')\n",
        "\n",
        "# API Headers\n",
        "Translate = {\"Authorization\": f\"Bearer {Transalate_token}\"}\n",
        "Image_generation = {\"Authorization\": f\"Bearer {Image_Token}\"}\n",
        "Content_generation = {\n",
        "    \"Authorization\": f\"Bearer {Content_Token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "Image_Prompt = {\n",
        "    \"Authorization\": f\"Bearer {Image_prompt_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Translation Model API URL (Tamil to English)\n",
        "translation_url = \"https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt\"\n",
        "\n",
        "# Text-to-Image Model API URLs\n",
        "image_generation_urls = {\n",
        "    \"black-forest-labs/FLUX.1-schnell\": \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\",\n",
        "    \"CompVis/stable-diffusion-v1-4\": \"https://api-inference.huggingface.co/models/CompVis/stable-diffusion-v1-4\",\n",
        "    \"black-forest-labs/FLUX.1-dev\": \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
        "}\n",
        "\n",
        "# Default image generation model\n",
        "default_image_model = \"black-forest-labs/FLUX.1-schnell\"\n",
        "\n",
        "# Content generation models\n",
        "content_models = {\n",
        "    \"llama-3.1-70b-versatile\": \"llama-3.1-70b-versatile\",\n",
        "    \"llama3-8b-8192\": \"llama3-8b-8192\",\n",
        "    \"gemma2-9b-it\": \"gemma2-9b-it\",\n",
        "    \"mixtral-8x7b-32768\": \"mixtral-8x7b-32768\"\n",
        "}\n",
        "\n",
        "# Default content generation model\n",
        "default_content_model = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "# Function to query Hugging Face translation model\n",
        "def translate_text(text):\n",
        "    payload = {\"inputs\": text}\n",
        "    response = requests.post(translation_url, headers=Translate, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        translated_text = result[0]['generated_text']\n",
        "        return translated_text\n",
        "    else:\n",
        "        st.error(f\"Translation Error {response.status_code}: {response.text}\")\n",
        "        st.write(f'_________________________________________________________')\n",
        "        st.write(f'There is a issue in Transulation model,Please try with some other inputs or please try again laterüò•üò•üò•')\n",
        "        return None\n",
        "\n",
        "# Function to query Groq content generation model\n",
        "def generate_content(english_text, max_tokens, temperature, model):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative and insightful writer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Write educational content about {english_text} within {max_tokens} tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=Content_generation)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Content Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate image prompt\n",
        "def generate_image_prompt(english_text):\n",
        "    payload = {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a professional Text to image prompt generator.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Create a text to image generation prompt about {english_text} within 30 tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": 30\n",
        "    }\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", json=payload, headers=Image_Prompt)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Prompt Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate an image from the prompt\n",
        "def generate_image(image_prompt, model_url):\n",
        "    data = {\"inputs\": image_prompt}\n",
        "    response = requests.post(model_url, headers=Image_generation, json=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        st.error(f\"Image Generation Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    # Custom CSS for background, borders, and other styling\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        body {\n",
        "            background-image: url('https://wallpaperaccess.com/full/1567666.png');\n",
        "            background-size: cover;\n",
        "        }\n",
        "        .reportview-container {\n",
        "            background: rgba(255, 255, 255, 0.85);\n",
        "            padding: 2rem;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        .result-container {\n",
        "            border: 2px solid #4CAF50;\n",
        "            padding: 20px;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 20px;\n",
        "            animation: fadeIn 2s ease;\n",
        "        }\n",
        "        @keyframes fadeIn {\n",
        "            0% { opacity: 0; }\n",
        "            100% { opacity: 1; }\n",
        "        }\n",
        "        .stButton button {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "            border-radius: 10px;\n",
        "            padding: 10px;\n",
        "        }\n",
        "        .stButton button:hover {\n",
        "            background-color: #45a049;\n",
        "            transform: scale(1.05);\n",
        "            transition: 0.2s ease-in-out;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\", unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    st.title(\"üÖ∞Ô∏è‚ÑπÔ∏è FusionMind ‚û°Ô∏è Multimodal\")\n",
        "\n",
        "    # Sidebar for temperature, token adjustment, and model selection\n",
        "    st.sidebar.header(\"Settings\")\n",
        "    temperature = st.sidebar.slider(\"Select Temperature\", 0.1, 1.0, 0.7)\n",
        "    max_tokens = st.sidebar.slider(\"Max Tokens for Content Generation\", 100, 400, 200)\n",
        "\n",
        "    # Content generation model selection\n",
        "    content_model = st.sidebar.selectbox(\"Select Content Generation Model\", list(content_models.keys()), index=0)\n",
        "\n",
        "    # Image generation model selection\n",
        "    image_model = st.sidebar.selectbox(\"Select Image Generation Model\", list(image_generation_urls.keys()), index=0)\n",
        "\n",
        "    # Reminder about model availability\n",
        "    st.sidebar.warning(\"Note: Based on availability, some models might not work. Please try another model if an error occurs.By default the perfect model is selected try with it and then experiment with different models\")\n",
        "\n",
        "    # Suggested inputs\n",
        "    st.write(\"## Suggested Inputs\")\n",
        "    suggestions = [\"‡Æ§‡Æ∞‡Æµ‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\", \"‡Æâ‡Æ≥‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\", \"‡Æ∞‡Ææ‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æµ‡Øá‡Æ≤‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\"]\n",
        "    selected_suggestion = st.selectbox(\"Select a suggestion or enter your own:\", [\"\"] + suggestions)\n",
        "\n",
        "    # Input box for user\n",
        "    tamil_input = st.text_input(\"Enter Tamil text (or select a suggestion):\", selected_suggestion)\n",
        "\n",
        "    if st.button(\"Generate\"):\n",
        "        # Step 1: Translation (Tamil to English)\n",
        "        if tamil_input:\n",
        "            st.write(\"### Translated English Text:\")\n",
        "            english_text = translate_text(tamil_input)\n",
        "            if english_text:\n",
        "                st.success(english_text)\n",
        "\n",
        "                # Step 2: Generate Content\n",
        "                st.write(\"### Generated Content:\")\n",
        "                with st.spinner('Generating content...'):\n",
        "                    content_output = generate_content(english_text, max_tokens, temperature, content_models[content_model])\n",
        "                    if content_output:\n",
        "                        st.success(content_output)\n",
        "\n",
        "                # Step 3: Generate Image from the prompt\n",
        "                st.write(\"### Generated Image:\")\n",
        "                with st.spinner('Generating image...'):\n",
        "                    image_prompt = generate_image_prompt(english_text)\n",
        "                    image_data = generate_image(image_prompt, image_generation_urls[image_model])\n",
        "                    if image_data:\n",
        "                        st.image(image_data, caption=\"Generated Image\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AnR50PdIepY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Code with the User Guide"
      ],
      "metadata": {
        "id": "yeL6R5UFZ_Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Fetch Hugging Face and Groq API keys from secrets\n",
        "Transalate_token = os.getenv('Translate')\n",
        "Image_Token = os.getenv('Image_generation')\n",
        "Content_Token = os.getenv('ContentGeneration')\n",
        "Image_prompt_token = os.getenv('Prompt_generation')\n",
        "\n",
        "# API Headers\n",
        "Translate = {\"Authorization\": f\"Bearer {Transalate_token}\"}\n",
        "Image_generation = {\"Authorization\": f\"Bearer {Image_Token}\"}\n",
        "Content_generation = {\n",
        "    \"Authorization\": f\"Bearer {Content_Token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "Image_Prompt = {\n",
        "    \"Authorization\": f\"Bearer {Image_prompt_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Translation Model API URL (Tamil to English)\n",
        "translation_url = \"https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt\"\n",
        "\n",
        "# Text-to-Image Model API URLs\n",
        "image_generation_urls = {\n",
        "    \"black-forest-labs/FLUX.1-schnell\": \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\",\n",
        "    \"CompVis/stable-diffusion-v1-4\": \"https://api-inference.huggingface.co/models/CompVis/stable-diffusion-v1-4\",\n",
        "    \"black-forest-labs/FLUX.1-dev\": \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
        "}\n",
        "\n",
        "# Default image generation model\n",
        "default_image_model = \"black-forest-labs/FLUX.1-schnell\"\n",
        "\n",
        "# Content generation models\n",
        "content_models = {\n",
        "    \"llama-3.1-70b-versatile\": \"llama-3.1-70b-versatile\",\n",
        "    \"llama3-8b-8192\": \"llama3-8b-8192\",\n",
        "    \"gemma2-9b-it\": \"gemma2-9b-it\",\n",
        "    \"mixtral-8x7b-32768\": \"mixtral-8x7b-32768\"\n",
        "}\n",
        "\n",
        "# Default content generation model\n",
        "default_content_model = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "# Function to query Hugging Face translation model\n",
        "def translate_text(text):\n",
        "    payload = {\"inputs\": text}\n",
        "    response = requests.post(translation_url, headers=Translate, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        translated_text = result[0]['generated_text']\n",
        "        return translated_text\n",
        "    else:\n",
        "        st.error(f\"Translation Error {response.status_code}: {response.text}\")\n",
        "        st.write(f'_________________________________________________________')\n",
        "        st.write(f'There is a issue in Transulation model,Please try with some other inputs or please try again laterüò•üò•üò•')\n",
        "        return None\n",
        "\n",
        "# Function to query Groq content generation model\n",
        "def generate_content(english_text, max_tokens, temperature, model):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative and insightful writer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Write educational content about {english_text} within {max_tokens} tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=Content_generation)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Content Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate image prompt\n",
        "def generate_image_prompt(english_text):\n",
        "    payload = {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a professional Text to image prompt generator.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Create a text to image generation prompt about {english_text} within 30 tokens.\"}\n",
        "        ],\n",
        "        \"max_tokens\": 30\n",
        "    }\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", json=payload, headers=Image_Prompt)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        st.error(f\"Prompt Generation Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate an image from the prompt\n",
        "def generate_image(image_prompt, model_url):\n",
        "    data = {\"inputs\": image_prompt}\n",
        "    response = requests.post(model_url, headers=Image_generation, json=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        st.error(f\"Image Generation Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# User Guide Section\n",
        "def show_user_guide():\n",
        "    st.title(\"FusionMind User Guide\")\n",
        "    st.write(\"\"\"\n",
        "    ### Welcome to the FusionMind User Guide!\n",
        "\n",
        "### How to use this app:\n",
        "\n",
        "1. **Input Tamil Text**:\n",
        "   - You can either select one of the suggested Tamil phrases or input your own text. The app primarily focuses on Tamil inputs, but it supports a wide range of other languages as well (see the list below).\n",
        "\n",
        "2. **Generate Translations**:\n",
        "   - Once you've input your text, the app will automatically translate it to English. The translation model is a **many-to-one model**, meaning it can take input from various languages and translate it into English.\n",
        "\n",
        "3. **Generate Educational Content**:\n",
        "   - After translating the text into English, the app will generate **educational content** based on the translated input. You can adjust the creativity of the content generation using the temperature slider, and control the length of the output with the token limit setting.\n",
        "\n",
        "4. **Generate Images**:\n",
        "   - In addition to generating content, the app can also generate an **image** related to the translated content. You don‚Äôt need to worry about creating complex image prompts‚ÄîFusionMind includes an automatic **image prompt generator** that will convert your input into a well-defined image prompt, ensuring better image generation results.\n",
        "\n",
        "---\n",
        "\n",
        "### Features:\n",
        "\n",
        "- **Multilingual Translation**:\n",
        "   - FusionMind supports a **many-to-one translation model**, so you can input text in a wide variety of languages, not just Tamil. Below are the supported languages:\n",
        "\n",
        "     - **Arabic (ar_AR)**, **Czech (cs_CZ)**, **German (de_DE)**, **English (en_XX)**, **Spanish (es_XX)**, **Estonian (et_EE)**, **Finnish (fi_FI)**, **French (fr_XX)**, **Gujarati (gu_IN)**, **Hindi (hi_IN)**, **Italian (it_IT)**, **Japanese (ja_XX)**, **Kazakh (kk_KZ)**, **Korean (ko_KR)**, **Lithuanian (lt_LT)**, **Latvian (lv_LV)**, **Burmese (my_MM)**, **Nepali (ne_NP)**, **Dutch (nl_XX)**, **Romanian (ro_RO)**, **Russian (ru_RU)**, **Sinhala (si_LK)**, **Turkish (tr_TR)**, **Vietnamese (vi_VN)**, **Chinese (zh_CN)**, **Afrikaans (af_ZA)**, **Azerbaijani (az_AZ)**, **Bengali (bn_IN)**, **Persian (fa_IR)**, **Hebrew (he_IL)**, **Croatian (hr_HR)**, **Indonesian (id_ID)**, **Georgian (ka_GE)**, **Khmer (km_KH)**, **Macedonian (mk_MK)**, **Malayalam (ml_IN)**, **Mongolian (mn_MN)**, **Marathi (mr_IN)**, **Polish (pl_PL)**, **Pashto (ps_AF)**, **Portuguese (pt_XX)**, **Swedish (sv_SE)**, **Swahili (sw_KE)**, **Tamil (ta_IN)**, **Telugu (te_IN)**, **Thai (th_TH)**, **Tagalog (tl_XX)**, **Ukrainian (uk_UA)**, **Urdu (ur_PK)**, **Xhosa (xh_ZA)**, **Galician (gl_ES)**, **Slovene (sl_SI)**.\n",
        "\n",
        "- **Temperature Adjustment**:\n",
        "   - You can adjust the **temperature** of the content generation. A **higher temperature** makes the content more creative and varied, while a **lower temperature** generates more focused and deterministic responses.\n",
        "\n",
        "- **Token Limit**:\n",
        "   - Set the **maximum number of tokens** for content generation. This allows you to control the length of the generated educational content.\n",
        "\n",
        "- **Automatic Retries**:\n",
        "   - If a translation request fails due to any reason, the app is designed to **automatically retry**, ensuring a smooth experience.\n",
        "\n",
        "- **Auto-Generated Image Prompts**:\n",
        "   - One of the unique features of FusionMind is the **auto-generated image prompts**. Even if you're not experienced in creating detailed prompts for image generation, the app will take care of this for you. It automatically converts the translated text or content into a well-defined prompt that produces more accurate and high-quality images.\n",
        "\n",
        "---\n",
        "\n",
        "Enjoy the multimodal experience with **FusionMind** and explore its powerful translation, content generation, and image generation features!\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    # Sidebar Menu\n",
        "    st.sidebar.title(\"FusionMind Options\")\n",
        "    page = st.sidebar.radio(\"Select a page:\", [\"Main App\", \"User Guide\"])\n",
        "\n",
        "    if page == \"User Guide\":\n",
        "        show_user_guide()\n",
        "        return\n",
        "\n",
        "    # Custom CSS for background, borders, and other styling\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        body {\n",
        "            background-image: url('https://wallpapercave.com/wp/wp4008910.jpg');\n",
        "            background-size: cover;\n",
        "        }\n",
        "        .reportview-container {\n",
        "            background: rgba(255, 255, 255, 0.85);\n",
        "            padding: 2rem;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        .result-container {\n",
        "            border: 2px solid #4CAF50;\n",
        "            padding: 20px;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 20px;\n",
        "            animation: fadeIn 2s ease;\n",
        "        }\n",
        "        @keyframes fadeIn {\n",
        "            0% { opacity: 0; }\n",
        "            100% { opacity: 1; }\n",
        "        }\n",
        "        .stButton button {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "            border-radius: 10px;\n",
        "            padding: 10px;\n",
        "        }\n",
        "        .stButton button:hover {\n",
        "            background-color: #45a049;\n",
        "            transform: scale(1.05);\n",
        "            transition: 0.2s ease-in-out;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\", unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    st.title(\"üÖ∞Ô∏è‚ÑπÔ∏è FusionMind ‚û°Ô∏è Multimodal\")\n",
        "\n",
        "    # Sidebar for temperature, token adjustment, and model selection\n",
        "    st.sidebar.header(\"Settings\")\n",
        "    temperature = st.sidebar.slider(\"Select Temperature\", 0.1, 1.0, 0.7)\n",
        "    max_tokens = st.sidebar.slider(\"Max Tokens for Content Generation\", 100, 400, 200)\n",
        "\n",
        "    # Content generation model selection\n",
        "    content_model = st.sidebar.selectbox(\"Select Content Generation Model\", list(content_models.keys()), index=0)\n",
        "\n",
        "    # Image generation model selection\n",
        "    image_model = st.sidebar.selectbox(\"Select Image Generation Model\", list(image_generation_urls.keys()), index=0)\n",
        "\n",
        "    # Reminder about model availability\n",
        "    st.sidebar.warning(\"Note: Based on availability, some models might not work. Please try another model if an error occurs.By default the perfect model is selected try with it and then experiment with different models\")\n",
        "\n",
        "    # Suggested inputs\n",
        "    st.write(\"## Suggested Inputs\")\n",
        "    suggestions = [\"‡Æ§‡Æ∞‡Æµ‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\", \"‡Æâ‡Æ≥‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\", \"‡Æ∞‡Ææ‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æµ‡Øá‡Æ≤‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\"]\n",
        "    selected_suggestion = st.selectbox(\"Select a suggestion or enter your own:\", [\"\"] + suggestions)\n",
        "\n",
        "    # Input box for user\n",
        "    tamil_input = st.text_input(\"Enter Tamil text (or select a suggestion):\", selected_suggestion)\n",
        "\n",
        "    if st.button(\"Generate\"):\n",
        "        # Step 1: Translation (Tamil to English)\n",
        "        if tamil_input:\n",
        "            st.write(\"### Translated English Text:\")\n",
        "            english_text = translate_text(tamil_input)\n",
        "            if english_text:\n",
        "                st.success(english_text)\n",
        "\n",
        "                # Step 2: Generate Educational Content\n",
        "                st.write(\"### Generated Educational Content:\")\n",
        "                with st.spinner('Generating content...'):\n",
        "                    content_output = generate_content(english_text, max_tokens, temperature, content_models[content_model])\n",
        "                    if content_output:\n",
        "                        st.success(content_output)\n",
        "\n",
        "                # Step 3: Generate Image from the prompt\n",
        "                st.write(\"### Generated Image:\")\n",
        "                with st.spinner('Generating image...'):\n",
        "                    image_prompt = generate_image_prompt(english_text)\n",
        "                    image_data = generate_image(image_prompt, image_generation_urls[image_model])\n",
        "                    if image_data:\n",
        "                        st.image(image_data, caption=\"Generated Image\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RPeQjHjKaDPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}